SECTION 1. SHORT TITLE.
This Act may be cited as the “Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act” or the “DISCOURSE Act”.SEC. 2. CONTENT MODERATION, CREATION AND DEVELOPMENT, AND DISTRIBUTION.
(a) Treatment As Publisher Or Speaker Contingent On Content Management Practices.—Section 230 of the Communications Act of 1934 (47 U.S.C. 230) is amended—
(1) in subsection (c)(1)—
(A) by striking “No provider” and inserting the following:
“(A) IN GENERAL.—Subject to subparagraph (B), no provider”; and
(B) by adding at the end the following:“(B) NOTIFICATION OF PARENTAL CONTROL PROTECTIONS.—Subparagraph (A) shall not apply to a provider of an interactive computer service with a dominant market share that violates subsection (d).”; and
(2) in subsection (f)—
(A) in paragraph (3)—
(i) by striking “The term” and inserting the following:
“(A) IN GENERAL.—The term”; and
(ii) by adding at the end the following:“(B) CONTENT MODERATION.—If an interactive computer service provider with a dominant market share—
“(i) engages in a content moderation activity that reasonably appears to express, promote, or suppress a discernible viewpoint for a reason that is not protected from liability under subsection (c)(2), including reducing or eliminating the ability of an information content provider to earn revenue, with respect to any information, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or
“(ii) engages in a pattern or practice of content moderation activity that reasonably appears to express, promote, or suppress a discernible viewpoint for a reason that is not protected from liability under subsection (c)(2), including reducing or eliminating the ability of an information content provider to earn revenue, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.“(C) USE OF TARGETED ALGORITHMIC AMPLIFICATION.—
“(i) IN GENERAL.—If an interactive computer service provider with a dominant market share—
“(I) amplifies information provided by an information content provider by using an algorithm or other automated computer process to target the information directly to users without the request of the sending or receiving user, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or
“(II) engages in a pattern or practice of amplifying information provided by an information content provider by using an algorithm or other automated computer process to target the information directly to users without the request of the sending or receiving user, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.“(ii) EXCEPTIONS.—Clause (i) shall not apply to the use of an algorithm or other computer process to—
“(I) amplify or target directly to a user any information that is the result of a search function performed by the user; or
“(II) sort data chronologically or alphabetically.“(D) INFORMATION CREATION OR DEVELOPMENT.—If an interactive computer service provider with a dominant market share—
“(i) solicits, comments upon, funds, or affirmatively and substantively contributes to, modifies, or alters information provided by an information content provider, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or
“(ii) engages in a pattern or practice of soliciting, commenting upon, funding, or affirmatively and substantively contributing to, modifying, or altering information provided by an information content provider, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.”; and(B) by adding at the end the following:
“(5) CONTENT MODERATION ACTIVITY.—The term ‘content moderation activity’ means editing, deleting, throttling, limiting the reach of, reducing or eliminating the ability of an information content provider to earn revenue from, or commenting upon, information provided by an information content provider, or terminating or limiting an account or usership, if the activity is based on content-based criteria.
“(6) PATTERN OR PRACTICE.—The term ‘pattern or practice’ means any formal or informal policy or rule, whether created by a human or generated by a computer, as applied or used by an interactive computer service provider.”.(b) Clarifying Categories Of Objectionable Material.—Section 230(c)(2) of the Communications Act of 1934 (47 U.S.C. 230(c)(2)) is amended—
(1) in subparagraph (A)—
(A) by striking “considers to be” and inserting “has an objectively reasonable belief is”;
(B) by inserting “promoting terrorism or violent extremism,” after “violent,”; and
(C) by striking “or otherwise objectionable” and inserting “promoting self-harm, or unlawful”; and
(2) in subparagraph (B), by striking “paragraph (1)” and inserting “subparagraph (A)”.
(c) Religious Liberty Exception To Civil Liability Protections.—Section 230(c)(2) of the Communications Act of 1934 (47 U.S.C. 230(c)(2)), as amended by subsection (b), is amended—
(1) by redesignating subparagraphs (A) and (B) as clauses (i) and (ii), respectively, and adjusting the margins accordingly;
(2) by striking “No provider” and inserting the following:
“(A) IN GENERAL.—Except as provided in subparagraph (B), no provider”;
(3) in subparagraph (A)(ii), as so designated, by striking “subparagraph (A)” and inserting “clause (i)”; and
(4) by adding at the end the following:“(B) RELIGIOUS LIBERTY EXCEPTION.—Subparagraph (A) shall not apply to any action taken with respect to religious material in a manner that burdens the exercise of religion, as defined in section 5 of the Religious Freedom Restoration Act of 1993 (42 U.S.C. 2000bb –2).”.
(d) Disclosure Of Content Management Mechanisms And Practices.—Section 230(d) of the Communications Act of 1934 (47 U.S.C. 230(d)) is amended—
(1) by striking “A provider” and inserting the following:
“(1) PARENTAL CONTROL PROTECTIONS.—A provider”; and
(2) by adding at the end the following:“(2) DISCLOSURE OF CONTENT MANAGEMENT MECHANISMS AND PRACTICES.—
“(A) IN GENERAL.—A provider of an interactive computer service that provides the service through a mass-market offering to the public shall publicly disclose accurate information regarding the content moderation activity of the service, including editing, deleting, throttling, limiting the reach of, reducing or eliminating the ability of an information content provider to earn revenue from, or commenting upon, information provided by an information content provider, terminating or limiting an account or usership, and any other content moderation, promotion, and other curation practices, sufficient to enable—
“(i) consumers to make informed choices regarding the purchase and use of the service; and
“(ii) entrepreneurs and other small businesses to develop, market, and maintain offerings by means of the service.“(B) MANNER OF DISCLOSURE.—A provider of an interactive computer service shall make the disclosure under subparagraph (A)—
“(i) through a publicly available, easily accessible website; or
“(ii) by submitting the information described in that subparagraph to the Commission, which shall make the information available to the public through the website of the Commission.”.(e) Clarifying That Immunity Is An Affirmative Defense.—Section 230(c)(1) of the Communications Act of 1934 (47 U.S.C. 230(c)(1)), as amended by subsection (a)(1), is amended—
(1) in subparagraph (A), as so designated, by striking “subparagraph (B)” and inserting “subparagraphs (B) and (C)”; and
(2) by adding at the end the following:“(C) AFFIRMATIVE DEFENSE.—In a criminal or civil action against a provider or user of an interactive computer service that treats the provider or user as the publisher or speaker of any information, the provider or user shall bear the burden of proving that the provider or user is not an information content provider with respect to that information for purposes of subparagraph (A).”.