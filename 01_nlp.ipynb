{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ec636b-f361-4674-8c3e-c31a237e567e",
   "metadata": {},
   "source": [
    "# Importing AGORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "66a97ab5-5519-4be4-908d-a91c66e12707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('agora/documents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6b66b570-138e-4533-81a6-aaedd7c04aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This datasets consists of 650 AI-related bills in the USA.\n"
     ]
    }
   ],
   "source": [
    "print(f\"This datasets consists of {len(df)} AI-related bills in the USA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "703183e8-57a0-4eb8-9ae9-00b42feded9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'agora/fulltext/207.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/494.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/402.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/29.txt'\n"
     ]
    }
   ],
   "source": [
    "def add_full_text(agora_id):\n",
    "    text = None\n",
    "    try:\n",
    "        with open(f'agora/fulltext/{agora_id}.txt', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    return text\n",
    "df[\"full_text\"] = df[\"AGORA ID\"].apply(add_full_text)\n",
    "df.to_csv('agora_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc66fc-8044-4635-8a88-528b77b9ecf1",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fad0cc74-49e9-4f7d-b799-5e7f453d22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agora_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fe65441-866c-4ebb-bf1a-331bcf6643e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/noahfehr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/noahfehr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb2698ad-a6c0-45b0-ab8d-f67ac55383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    # stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return no_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bd4e3e9-68ab-492f-92fc-8916d43131cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d89b9fe-67c0-40aa-b4a0-1be9f67b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(df[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "498517cb-e485-4621-a4e0-d21a4d7a43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for doc in sample:\n",
    "    try:\n",
    "        sentences += get_sentences(doc)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33231422-e30c-45ef-88dd-82bad9fb6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce5f81ea-e033-4191-b815-0368332fd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               vector_size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cde24350-ae21-4282-ba21-63b0feaa6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('legal_words.txt') as file:\n",
    "    common_law_terms = file.read()\n",
    "common_law_terms = common_law_terms.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12b5cb0f-638f-49b8-8b85-d42834d75dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_law_similar = list()\n",
    "for term in common_law_terms:\n",
    "    try:\n",
    "        for similar_term, _ in w2v.wv.most_similar(term)[:5]:\n",
    "            common_law_similar.append(similar_term)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a43b779d-2a83-4ca1-9e65-f8904b6d056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_law_terms = common_law_terms + common_law_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618e8db-ca90-4680-8620-9edeaa6151ed",
   "metadata": {},
   "source": [
    "# Pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9edb3d7-52d1-49b2-9616-861a664302f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noahfehr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/noahfehr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ac26413-e772-44e6-b2f7-819406866af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in list(stop_words) + common_law_terms]\n",
    "        bigram = Phrases(tokens, min_count=5, threshold=10)\n",
    "        bigram_mod = Phraser(bigram)\n",
    "        tokens_with_bigrams = bigram_mod[tokens]\n",
    "        return ' '.join(tokens_with_bigrams)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25dba4f0-0b46-41c2-a617-676b2db23d5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[84], line 8\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop_words) \u001b[38;5;241m+\u001b[39m common_law_terms]\n\u001b[0;32m----> 8\u001b[0m bigram \u001b[38;5;241m=\u001b[39m Phrases(tokens, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      9\u001b[0m bigram_mod \u001b[38;5;241m=\u001b[39m Phraser(bigram)\n\u001b[1;32m     10\u001b[0m tokens_with_bigrams \u001b[38;5;241m=\u001b[39m bigram_mod[tokens]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gensim/models/phrases.py:569\u001b[0m, in \u001b[0;36mPhrases.__init__\u001b[0;34m(self, sentences, min_count, threshold, max_vocab_size, delimiter, progress_per, scoring, connector_words)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_vocab(sentences)\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gensim/models/phrases.py:648\u001b[0m, in \u001b[0;36mPhrases.add_vocab\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update model parameters with new `sentences`.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# Uses a separate vocab to collect the token counts from `sentences`.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# This consumes more RAM than merging new sentences into `self.vocab`\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# directly, but gives the new sentences a fighting chance to collect\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# sufficient counts, before being pruned out by the (large) accumulated\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# counts collected in previous learn_vocab runs.\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m min_reduce, vocab, total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learn_vocab(\n\u001b[1;32m    649\u001b[0m     sentences, max_vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter,\n\u001b[1;32m    650\u001b[0m     progress_per\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_per, connector_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnector_words,\n\u001b[1;32m    651\u001b[0m )\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_word_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_words\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gensim/models/phrases.py:596\u001b[0m, in \u001b[0;36mPhrases._learn_vocab\u001b[0;34m(sentences, max_vocab_size, delimiter, connector_words, progress_per)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     phrase_tokens \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain([start_token], in_between, [word])\n\u001b[0;32m--> 596\u001b[0m     joined_phrase_token \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mjoin(phrase_tokens)\n\u001b[1;32m    597\u001b[0m     vocab[joined_phrase_token] \u001b[38;5;241m=\u001b[39m vocab\u001b[38;5;241m.\u001b[39mget(joined_phrase_token, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    598\u001b[0m start_token, in_between \u001b[38;5;241m=\u001b[39m word, []  \u001b[38;5;66;03m# treat word as both end of a phrase AND beginning of another\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df[\"full_text_preprocessed\"] = df[\"full_text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "014fb31c-7cdd-475a-956c-848130f850ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"agora_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258f8fb-a218-44a8-826c-220b7a506fdb",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f33b6df-82e0-4bbb-ba08-844656194274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"agora_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c89260-d62e-4d65-81aa-f8257bbae2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m doc_term_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "doc_term_matrix = vectorizer.fit_transform(df[\"full_text_preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeb85c92-178d-4fd3-a89b-7b695a68fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'academia', 'academic', 'accelerate', 'access',\n",
       "       'accessible', 'accordance', 'account', 'accountability',\n",
       "       'accuracy', 'achieve', 'acquisition', 'across', 'acting', 'action',\n",
       "       'actions', 'activities', 'activity', 'actors', 'acts', 'actual',\n",
       "       'added', 'adding', 'addition', 'additional', 'address',\n",
       "       'administration', 'administrative', 'administrator', 'adopt',\n",
       "       'adoption', 'ads', 'advance', 'advanced', 'advancing', 'adverse',\n",
       "       'affairs', 'affect', 'affected', 'age', 'agencies', 'agency',\n",
       "       'agencys', 'agreement', 'agreements', 'agriculture', 'ai', 'air',\n",
       "       'aircraft', 'algorithm', 'algorithmic', 'algorithms', 'allow',\n",
       "       'alternative', 'amended', 'america', 'american', 'among', 'amount',\n",
       "       'amounts', 'analysis', 'annex', 'annual', 'another', 'applicable',\n",
       "       'applicant', 'application', 'applications', 'applied', 'apply',\n",
       "       'appointed', 'approach', 'approaches', 'appropriate',\n",
       "       'appropriated', 'appropriations', 'approval', 'approved', 'area',\n",
       "       'areas', 'armed', 'article', 'assembly', 'assess', 'assessment',\n",
       "       'assessments', 'assist', 'assistance', 'assistant', 'associated',\n",
       "       'atmospheric', 'attorney', 'audio', 'authorities', 'authority',\n",
       "       'authorization', 'authorized', 'automated', 'autonomous',\n",
       "       'availability', 'available', 'aviation', 'avoid', 'award',\n",
       "       'awards', 'based', 'basic', 'basis', 'beginning', 'behalf',\n",
       "       'benefit', 'benefits', 'best', 'better', 'bias', 'biological',\n",
       "       'biometric', 'board', 'bodies', 'body', 'border', 'budget',\n",
       "       'build', 'building', 'business', 'businesses', 'campaign',\n",
       "       'candidate', 'capabilities', 'capability', 'capable', 'capacity',\n",
       "       'care', 'carried', 'carry', 'carrying', 'case', 'cases',\n",
       "       'categories', 'cause', 'center', 'centers', 'certain',\n",
       "       'certification', 'chain', 'chains', 'chair', 'challenges',\n",
       "       'change', 'changes', 'chapter', 'characteristics', 'chief',\n",
       "       'child', 'children', 'china', 'circumstances', 'cited', 'civil',\n",
       "       'class', 'classification', 'classified', 'clear', 'clearly',\n",
       "       'cloud', 'code', 'collaboration', 'collected', 'collection',\n",
       "       'colleges', 'commerce', 'commercial', 'commission', 'committees',\n",
       "       'common', 'communication', 'communications', 'communities',\n",
       "       'companies', 'company', 'competent', 'competition', 'competitive',\n",
       "       'complete', 'compliance', 'comply', 'components', 'comprehensive',\n",
       "       'computational', 'computer', 'computing', 'concern', 'concerning',\n",
       "       'concerns', 'conditions', 'conduct', 'conducted', 'conducting',\n",
       "       'conformity', 'congress', 'congressional', 'consent', 'consider',\n",
       "       'consideration', 'considered', 'considers', 'consistent',\n",
       "       'consortium', 'construction', 'construed', 'consult',\n",
       "       'consultation', 'consumer', 'consumers', 'contact', 'containing',\n",
       "       'contains', 'content', 'contents', 'context', 'continue',\n",
       "       'contract', 'contracts', 'control', 'controlled', 'controls',\n",
       "       'cooperation', 'coordinate', 'coordination', 'cost', 'costs',\n",
       "       'could', 'council', 'countries', 'country', 'court', 'covered',\n",
       "       'create', 'created', 'creating', 'creation', 'credit', 'criminal',\n",
       "       'criteria', 'critical', 'current', 'customer', 'customers',\n",
       "       'cyber', 'cybersecurity', 'data', 'database', 'datasets', 'date',\n",
       "       'december', 'deceptive', 'decision', 'decisionmaking', 'decisions',\n",
       "       'deep', 'defense', 'defined', 'definition', 'definitions',\n",
       "       'definitionsin', 'degree', 'delivery', 'demonstrate',\n",
       "       'demonstration', 'department', 'departments', 'depiction',\n",
       "       'deployed', 'deployer', 'deployers', 'deployment', 'described',\n",
       "       'description', 'design', 'designated', 'designed', 'designee',\n",
       "       'detailed', 'detection', 'determination', 'determine',\n",
       "       'determined', 'develop', 'developed', 'developer', 'developers',\n",
       "       'developing', 'development', 'device', 'devices', 'different',\n",
       "       'digital', 'direct', 'directive', 'directly', 'director',\n",
       "       'disclose', 'disclosure', 'discrimination', 'dissemination',\n",
       "       'distributed', 'distribution', 'district', 'diverse', 'diversity',\n",
       "       'document', 'documentation', 'domestic', 'due', 'duties',\n",
       "       'economic', 'education', 'educational', 'effect', 'effective',\n",
       "       'effectively', 'effectiveness', 'effects', 'efficiency', 'efforts',\n",
       "       'eg', 'election', 'electronic', 'eligible', 'emergency',\n",
       "       'emerging', 'employee', 'employees', 'employer', 'employment',\n",
       "       'enable', 'enabling', 'enacted', 'encourage', 'end', 'energy',\n",
       "       'enforcement', 'engage', 'engaged', 'engagement', 'engineering',\n",
       "       'enhance', 'ensure', 'ensuring', 'enter', 'enterprises',\n",
       "       'entities', 'entity', 'environment', 'environmental',\n",
       "       'environments', 'equipment', 'equity', 'establish', 'established',\n",
       "       'establishing', 'establishment', 'et', 'etc', 'ethical', 'ethics',\n",
       "       'eu', 'european', 'evaluate', 'evaluation', 'evaluations', 'event',\n",
       "       'every', 'evidence', 'example', 'except', 'executive', 'existing',\n",
       "       'expand', 'expected', 'experience', 'expertise', 'experts',\n",
       "       'export', 'extent', 'external', 'facial', 'facilitate',\n",
       "       'facilities', 'facility', 'factors', 'federal', 'feedback',\n",
       "       'field', 'fields', 'final', 'financial', 'findings', 'fire',\n",
       "       'first', 'fiscal', 'five', 'focus', 'following', 'follows', 'food',\n",
       "       'force', 'foreign', 'form', 'forth', 'foundation', 'framework',\n",
       "       'frameworks', 'frontier', 'full', 'function', 'functions', 'fund',\n",
       "       'fundamental', 'funding', 'funds', 'future', 'gender', 'general',\n",
       "       'generalnot', 'generalpurpose', 'generalthe', 'generate',\n",
       "       'generated', 'generation', 'generative', 'given', 'global',\n",
       "       'goals', 'good', 'goods', 'governance', 'government',\n",
       "       'governments', 'grant', 'grants', 'greater', 'group', 'groups',\n",
       "       'guidance', 'guide', 'guidelines', 'hardware', 'harm', 'harmful',\n",
       "       'harms', 'head', 'heads', 'health', 'help', 'high', 'higher',\n",
       "       'highrisk', 'hiring', 'homeland', 'human', 'identifiable',\n",
       "       'identification', 'identified', 'identify', 'identifying',\n",
       "       'identity', 'image', 'images', 'impact', 'impacts', 'implement',\n",
       "       'implementation', 'implemented', 'implementing', 'important',\n",
       "       'improve', 'improving', 'include', 'included', 'includes',\n",
       "       'including', 'increase', 'independent', 'individual',\n",
       "       'individuals', 'industrial', 'industries', 'industry', 'influence',\n",
       "       'inform', 'information', 'infrastructure', 'initial', 'initiative',\n",
       "       'innovation', 'innovative', 'input', 'inserting', 'institute',\n",
       "       'institution', 'institutions', 'insurance', 'integrated',\n",
       "       'integration', 'integrity', 'intellectual', 'intelligent',\n",
       "       'intended', 'interagency', 'interest', 'interests', 'internal',\n",
       "       'international', 'internet', 'intimate', 'investigation',\n",
       "       'investment', 'involved', 'involving', 'issue', 'issued', 'issues',\n",
       "       'january', 'jurisdiction', 'key', 'knowledge', 'known', 'labor',\n",
       "       'laboratories', 'language', 'large', 'law', 'laws', 'lead',\n",
       "       'leadership', 'learning', 'least', 'legal', 'legislative', 'less',\n",
       "       'level', 'levels', 'leverage', 'liberties', 'life', 'lifecycle',\n",
       "       'like', 'likely', 'limitations', 'limited', 'list', 'listed',\n",
       "       'literacy', 'loan', 'local', 'located', 'machine', 'made',\n",
       "       'maintain', 'maintenance', 'major', 'make', 'makes', 'making',\n",
       "       'manage', 'management', 'managing', 'manner', 'manufacturing',\n",
       "       'many', 'market', 'material', 'materially', 'materials', 'matter',\n",
       "       'matters', 'meaning', 'means', 'measure', 'measures', 'mechanism',\n",
       "       'mechanisms', 'media', 'medical', 'meet', 'meeting', 'members',\n",
       "       'memorandum', 'mental', 'methods', 'metrics', 'military',\n",
       "       'minimum', 'mission', 'misuse', 'mitigate', 'mitigation', 'model',\n",
       "       'modeling', 'models', 'monitor', 'monitoring', 'months',\n",
       "       'multiple', 'nairr', 'name', 'national', 'natural', 'nature',\n",
       "       'necessary', 'needed', 'needs', 'network', 'networks', 'new',\n",
       "       'nonprofit', 'note', 'notice', 'notification', 'notified',\n",
       "       'nuclear', 'number', 'objectives', 'obligations', 'obtain',\n",
       "       'obtained', 'office', 'officer', 'officers', 'offices', 'official',\n",
       "       'officials', 'omb', 'one', 'ongoing', 'online', 'open',\n",
       "       'operating', 'operation', 'operational', 'operations', 'operator',\n",
       "       'operators', 'opportunities', 'opportunity', 'order',\n",
       "       'organization', 'organizations', 'others', 'otherwise', 'outcomes',\n",
       "       'output', 'outputs', 'overall', 'oversight', 'paragraph',\n",
       "       'paragraphs', 'part', 'participate', 'participation', 'particular',\n",
       "       'parties', 'partners', 'partnership', 'partnerships', 'party',\n",
       "       'patient', 'pay', 'payment', 'penalties', 'penalty', 'people',\n",
       "       'peoples', 'percent', 'perform', 'performance', 'period', 'person',\n",
       "       'personal', 'personnel', 'persons', 'physical', 'pilot', 'place',\n",
       "       'plan', 'planning', 'plans', 'platform', 'platforms', 'point',\n",
       "       'policies', 'policy', 'political', 'position', 'positions',\n",
       "       'possible', 'potential', 'power', 'powers', 'practicable',\n",
       "       'practice', 'practices', 'prescribed', 'president', 'prevent',\n",
       "       'primary', 'principles', 'prior', 'privacy', 'private',\n",
       "       'procedure', 'procedures', 'process', 'processes', 'processing',\n",
       "       'procurement', 'produced', 'product', 'production', 'products',\n",
       "       'professional', 'program', 'programs', 'progress', 'prohibited',\n",
       "       'prohibition', 'project', 'projects', 'promote', 'promoting',\n",
       "       'property', 'proposed', 'protect', 'protected', 'protection',\n",
       "       'protections', 'provenance', 'provide', 'provided', 'provider',\n",
       "       'providers', 'provides', 'providing', 'provision', 'provisions',\n",
       "       'public', 'publication', 'publicly', 'published', 'purpose',\n",
       "       'purposes', 'pursuant', 'qualified', 'quality', 'quantum',\n",
       "       'questions', 'range', 'rd', 'read', 'real', 'reasonable',\n",
       "       'reasonably', 'receive', 'received', 'recognition',\n",
       "       'recommendation', 'recommendations', 'record', 'records', 'reduce',\n",
       "       'reference', 'referred', 'regard', 'regarding', 'regulation',\n",
       "       'regulations', 'regulatory', 'related', 'relating', 'release',\n",
       "       'relevant', 'reliability', 'relief', 'report', 'reporting',\n",
       "       'reportnot', 'reports', 'representative', 'representatives',\n",
       "       'republic', 'request', 'requests', 'require', 'required',\n",
       "       'requirement', 'requirements', 'requires', 'research',\n",
       "       'researchers', 'resilience', 'resource', 'resources', 'respect',\n",
       "       'response', 'responsibilities', 'responsibility', 'responsible',\n",
       "       'result', 'results', 'review', 'right', 'rights', 'risk', 'risks',\n",
       "       'role', 'roles', 'rule', 'rules', 'safe', 'safeguards', 'safety',\n",
       "       'scale', 'school', 'science', 'sciences', 'scientific', 'scope',\n",
       "       'score', 'screening', 'sec', 'secretary', 'sections', 'sector',\n",
       "       'sectors', 'secure', 'security', 'seek', 'select', 'senior',\n",
       "       'sensitive', 'seq', 'serve', 'service', 'services', 'set', 'sets',\n",
       "       'sexual', 'share', 'shared', 'sharing', 'short', 'significant',\n",
       "       'similar', 'skills', 'small', 'smart', 'social', 'societal',\n",
       "       'society', 'software', 'solutions', 'source', 'sources', 'space',\n",
       "       'special', 'specific', 'specified', 'spectrum', 'speech', 'sports',\n",
       "       'staff', 'stakeholders', 'standard', 'standards', 'state',\n",
       "       'statement', 'status', 'stem', 'storage', 'strategic',\n",
       "       'strategies', 'strategy', 'strengthen', 'striking', 'structure',\n",
       "       'students', 'study', 'subdivision', 'subject', 'submission',\n",
       "       'submit', 'submitted', 'subsection', 'substantially', 'sufficient',\n",
       "       'summary', 'supply', 'support', 'supporting', 'surveillance',\n",
       "       'synthetic', 'system', 'systems', 'table', 'take', 'taken',\n",
       "       'taking', 'talent', 'task', 'tasks', 'team', 'teams', 'technical',\n",
       "       'techniques', 'technological', 'technologies', 'technology',\n",
       "       'term', 'terms', 'test', 'testing', 'text', 'thereof', 'third',\n",
       "       'threat', 'threats', 'throughout', 'time', 'timely', 'title',\n",
       "       'tool', 'tools', 'trade', 'train', 'training', 'transaction',\n",
       "       'transfer', 'transparency', 'transportation', 'treatment',\n",
       "       'tribal', 'trust', 'trustworthy', 'two', 'type', 'types',\n",
       "       'understand', 'understanding', 'union', 'unlawful', 'unless',\n",
       "       'unmanned', 'update', 'updated', 'upon', 'us', 'usc', 'use',\n",
       "       'used', 'user', 'users', 'uses', 'using', 'utilization',\n",
       "       'validation', 'value', 'vehicle', 'vehicles', 'vendor',\n",
       "       'verification', 'video', 'vii', 'violation', 'violations',\n",
       "       'visual', 'voice', 'voluntary', 'vulnerabilities', 'wagering',\n",
       "       'water', 'way', 'ways', 'weapon', 'weather', 'website', 'well',\n",
       "       'whole', 'whose', 'within', 'without', 'work', 'worker', 'workers',\n",
       "       'workforce', 'working', 'world', 'written', 'year', 'years'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d5eeb9c-ca2b-4fa2-8176-a19b68246889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 5, 0],\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 6, 1, 1],\n",
       "       ...,\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4a41d75-2490-40da-87a8-2f9722c9b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=200,\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    batch_size=128,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c328e794-e34b-496d-9660-90a11054b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = lda.fit_transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b81865c-1dc5-4eee-9e16-8d471b56659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "276a323c-6cc8-4e6d-9a8e-9e3d79e910d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words in each topic:\n",
      "\n",
      "Topic 1:\n",
      "information, covered, state, person, commission, means, including, individual, service, data\n",
      "\n",
      "Topic 2:\n",
      "ai, systems, data, use, system, risks, security, development, including, safety\n",
      "\n",
      "Topic 3:\n",
      "secretary, national, including, subsection, director, research, technology, program, defense, paragraph\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop words in each topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-10-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    print(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93ba190-0a50-4a98-9e21-b6c8ad761144",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = [f'Topic_{i+1}' for i in range(n_topics)]\n",
    "df_topics = pd.DataFrame(lda_output, columns=topic_columns)\n",
    "df = pd.concat([df, df_topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ae27df1-faec-484a-a9bb-0fc5b22fd151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example documents for each topic:\n",
      "\n",
      "Top documents for Topic 1:\n",
      "\\Text: enacted state utah b amended read b definitions used chapter child sexual abuse material means visual depiction including live performance photograph film video picture computer computergenerated imag...\n",
      "Topic 1 probability: 0.999\n",
      "\\Text: relating elections amending enacting sections campaign reporting adding disclaimer requirements advertisements containing materially deceptive media creating crime distributing entering agreement anot...\n",
      "Topic 1 probability: 0.999\n",
      "\\Text: short title cited tools address known exploitation immobilizing technological deepfakes websites networks take sec criminal prohibition intentional disclosure nonconsensual intimate visual depictions ...\n",
      "Topic 1 probability: 0.999\n",
      "\n",
      "Top documents for Topic 2:\n",
      "\\Text: hiroshima process international code conduct organizations developing advanced ai systems basis international guiding principles organizations developing advanced ai systems international code conduct...\n",
      "Topic 2 probability: 0.999\n",
      "\\Text: background tangible global leadership european union provide scalable sciencebased methods advance trustworthy approaches ai serve people responsible equitable beneficial ways effective risk managemen...\n",
      "Topic 2 probability: 0.999\n",
      "\\Text: general assembly opening declarations omitted resolves bridge digital divides within countries resolves promote safe secure trustworthy systems accelerate progress towards full realization agenda sust...\n",
      "Topic 2 probability: 0.999\n",
      "\n",
      "Top documents for Topic 3:\n",
      "\\Text: sec strengthening mobility revolutionizing transportation grant program definitionsin eligible entitythe term eligible entity means state b political subdivision state c tribal government public trans...\n",
      "Topic 3 probability: 0.999\n",
      "\\Text: promote leadership technical standards directing national institute standards technology department state take certain actions encourage enable participation developing standards specifications critic...\n",
      "Topic 3 probability: 0.999\n",
      "\\Text: sec centers excellence food agriculture conservation trade usc amended striking subsections b c inserting following centers excellence generalthe secretary agriculture establish least one center excel...\n",
      "Topic 3 probability: 0.998\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample documents for each topic:\")\n",
    "for topic_idx in range(n_topics):\n",
    "    print(f\"\\nTop documents for Topic {topic_idx + 1}:\")\n",
    "    top_docs = df.nlargest(3, f'Topic_{topic_idx+1}')\n",
    "    for idx, row in top_docs.iterrows():\n",
    "        print(f\"\\Text: {row['full_text_preprocessed'][:200]}...\")\n",
    "        print(f\"Topic {topic_idx + 1} probability: {row[f'Topic_{topic_idx+1}']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef2ab5-44dd-435c-8e51-29d7c8fc4c4b",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22cd14ea-e9c1-43ea-9af1-909c2a8ff785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilx\\anaconda3\\envs\\nlp_lss\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "485eee2a-6b9a-4543-8425-0ef17b89ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean', prediction_data=True)\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    min_topic_size=10,     # Merges tiny topics into larger ones, when set to 20 only 2 topics\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abdbdad7-5e8d-4638-b6fc-fc4d12a83541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 12:20:26,124 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 21/21 [01:27<00:00,  4.14s/it]\n",
      "2025-05-28 12:21:54,834 - BERTopic - Embedding - Completed ✓\n",
      "2025-05-28 12:21:54,837 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-28 12:21:58,363 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-28 12:21:58,366 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-05-28 12:21:58,557 - BERTopic - Cluster - Completed ✓\n",
      "2025-05-28 12:21:58,572 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-05-28 12:22:00,691 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(df['full_text_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8984981-4e57-4e50-aab2-5af9bec8c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words for each topic:\n",
      "\n",
      "Topic 0:\n",
      "defense, education, director, program, research, secretary, subsection, national, including, paragraph\n",
      "\n",
      "Topic 1:\n",
      "ai, systems, system, data, use, article, security, risks, model, development\n",
      "\n",
      "Topic 2:\n",
      "energy, research, secretary, national, program, including, weather, development, technologies, data\n",
      "\n",
      "Topic 3:\n",
      "person, individual, election, media, image, audio, means, visual, sexual, video\n",
      "\n",
      "Topic 4:\n",
      "automated, system, data, decision, information, use, state, employer, used, systems\n",
      "\n",
      "Topic 5:\n",
      "covered, foreign, entity, president, secretary, security, regulations, term, national, activity\n",
      "\n",
      "Topic 6:\n",
      "health, care, plan, services, medical, patient, ai, use, program, benefits\n",
      "\n",
      "Topic 7:\n",
      "commission, agency, council, data, digital, criticalimpact, chief, government, state, including\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop words for each topic:\")\n",
    "for topic_id in topic_model.get_topics():\n",
    "    if topic_id != -1:  # Skip the outlier topic (-1)\n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        print(f\"\\nTopic {topic_id}:\")\n",
    "        print(\", \".join([word for word, _ in words[:10]]))  # Show top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cef1eca-5d54-414b-9041-382fb256af66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m BERT_probs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(probs, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT_topic7\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, BERT_probs], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "BERT_probs = pd.DataFrame(probs, columns=['BERT_topic0', 'BERT_topic1', 'BERT_topic2', 'BERT_topic3', 'BERT_topic4', 'BERT_topic5', 'BERT_topic6', 'BERT_topic7'])\n",
    "df = pd.concat([df, BERT_probs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b85e18c-448e-463e-90a9-41c7499eb07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   9,  20,  22,  25,  26,  34,  35,  36,  37,  38,  39,  40,\n",
       "        41,  42,  43,  44,  45,  53,  54,  56,  82,  83,  91,  92,  93,\n",
       "       101, 105, 107, 116, 117, 123, 132, 137, 143, 149, 167, 174, 175,\n",
       "       176, 178, 180, 181, 182, 183, 184, 185, 187, 188, 189, 195, 198,\n",
       "       210, 218, 220, 222, 225, 229, 232, 234, 238, 239, 243, 246, 248,\n",
       "       251, 252, 253, 261, 269, 271, 272, 283, 290, 291, 293, 296, 297,\n",
       "       298, 301, 302, 306, 307, 310, 315, 316, 317, 318, 321, 324, 325,\n",
       "       337, 340, 341, 343, 344, 353, 357, 358, 361, 373, 374, 381, 386,\n",
       "       390, 399, 400, 402, 411, 414, 416, 417, 423, 425, 427, 430, 432,\n",
       "       437, 442, 443, 448, 450, 452, 454, 456, 460, 462, 464, 467, 470,\n",
       "       472, 474, 476, 478, 479, 486, 488, 490, 495, 498, 499, 502, 503,\n",
       "       504, 505, 507, 508, 509, 514, 515, 516, 517, 518, 519, 527, 532,\n",
       "       533, 534, 535, 537, 547, 548, 552, 554, 555, 558, 559, 566, 568,\n",
       "       569, 570, 582, 583, 584, 585, 586, 588, 589, 600, 602, 605, 606,\n",
       "       611, 613, 614, 615, 616, 617, 618, 621, 623, 641, 645, 646, 647],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_topic_indices = np.where(np.array(topics) == -1)[0]\n",
    "dummy_topic_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d150819-3491-4925-892d-d33c7cfb2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(dummy_topic_indices)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "20fa6756-3381-4c02-914c-b2128df1124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('agora_topic_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bb8297d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression excluding all of the content-related columns:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         enacted_binary   R-squared:                       0.528\n",
      "Model:                            OLS   Adj. R-squared:                  0.511\n",
      "Method:                 Least Squares   F-statistic:                     30.87\n",
      "Date:                Thu, 29 May 2025   Prob (F-statistic):           1.91e-54\n",
      "Time:                        12:38:57   Log-Likelihood:                -136.32\n",
      "No. Observations:                 401   AIC:                             302.6\n",
      "Df Residuals:                     386   BIC:                             362.5\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================================\n",
      "                                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "const                                             2.1859      0.204     10.714      0.000       1.785       2.587\n",
      "Primarily applies to the private sector          -0.3197      0.069     -4.647      0.000      -0.455      -0.184\n",
      "BERT_topic0                                       0.1908      0.171      1.117      0.265      -0.145       0.527\n",
      "BERT_topic1                                      -0.3135      0.232     -1.352      0.177      -0.769       0.143\n",
      "BERT_topic2                                      -0.2939      0.174     -1.685      0.093      -0.637       0.049\n",
      "BERT_topic3                                      -0.0861      0.176     -0.488      0.626      -0.433       0.261\n",
      "BERT_topic4                                      -0.4381      0.176     -2.487      0.013      -0.784      -0.092\n",
      "BERT_topic5                                      -0.3624      0.175     -2.072      0.039      -0.706      -0.018\n",
      "BERT_topic6                                      -0.3919      0.174     -2.249      0.025      -0.735      -0.049\n",
      "BERT_topic7                                      -0.3894      0.174     -2.244      0.025      -0.730      -0.048\n",
      "gov_category_consolidated_blue_state             -0.2954      0.119     -2.483      0.013      -0.529      -0.061\n",
      "gov_category_consolidated_federal_legislative    -0.7793      0.108     -7.197      0.000      -0.992      -0.566\n",
      "gov_category_consolidated_purple_state           -0.3349      0.143     -2.344      0.020      -0.616      -0.054\n",
      "gov_category_consolidated_red_state              -0.0697      0.128     -0.543      0.587      -0.322       0.183\n",
      "datedummy                                        -0.0006   4.82e-05    -11.439      0.000      -0.001      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        0.242   Durbin-Watson:                   0.840\n",
      "Prob(Omnibus):                  0.886   Jarque-Bera (JB):                0.367\n",
      "Skew:                          -0.011   Prob(JB):                        0.832\n",
      "Kurtosis:                       2.854   Cond. No.                     4.84e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.84e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cc9bcc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ed3c15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.318874\n",
      "         Iterations 8\n",
      "\n",
      "Logit regression excluding all of the content-related columns, with interaction terms for datedummy on all gov_category vars:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:         enacted_binary   No. Observations:                  401\n",
      "Model:                          Logit   Df Residuals:                      386\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Thu, 29 May 2025   Pseudo R-squ.:                  0.5331\n",
      "Time:                        12:46:37   Log-Likelihood:                -127.87\n",
      "converged:                       True   LL-Null:                       -273.89\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.396e-54\n",
      "=================================================================================================================\n",
      "                                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "const                                            15.0823      2.328      6.478      0.000      10.519      19.645\n",
      "Primarily applies to the private sector          -3.8887      0.859     -4.529      0.000      -5.571      -2.206\n",
      "BERT_topic0                                       1.0761      1.515      0.710      0.477      -1.893       4.045\n",
      "BERT_topic1                                      -0.8131      3.005     -0.271      0.787      -6.703       5.077\n",
      "BERT_topic2                                      -1.8058      1.608     -1.123      0.262      -4.958       1.347\n",
      "BERT_topic3                                      -0.8878      1.692     -0.525      0.600      -4.204       2.429\n",
      "BERT_topic4                                      -3.4418      1.595     -2.158      0.031      -6.568      -0.316\n",
      "BERT_topic5                                      -2.7043      1.803     -1.500      0.134      -6.238       0.829\n",
      "BERT_topic6                                      -3.3581      1.742     -1.928      0.054      -6.772       0.056\n",
      "BERT_topic7                                      -3.0868      1.717     -1.798      0.072      -6.452       0.278\n",
      "gov_category_consolidated_blue_state             -2.1842      1.249     -1.748      0.080      -4.633       0.265\n",
      "gov_category_consolidated_federal_legislative    -6.1960      1.230     -5.035      0.000      -8.608      -3.784\n",
      "gov_category_consolidated_purple_state           -2.8287      1.357     -2.085      0.037      -5.488      -0.170\n",
      "gov_category_consolidated_red_state               0.3159      1.550      0.204      0.838      -2.721       3.353\n",
      "datedummy                                        -0.0053      0.001     -7.649      0.000      -0.007      -0.004\n",
      "=================================================================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.355539\n",
      "         Iterations 8\n",
      "\n",
      "Logit regression using only significant variables (p < 0.05):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:         enacted_binary   No. Observations:                  401\n",
      "Model:                          Logit   Df Residuals:                      392\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Thu, 29 May 2025   Pseudo R-squ.:                  0.4795\n",
      "Time:                        12:46:37   Log-Likelihood:                -142.57\n",
      "converged:                       True   LL-Null:                       -273.89\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.606e-52\n",
      "=================================================================================================================\n",
      "                                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "const                                            14.7277      1.614      9.124      0.000      11.564      17.891\n",
      "Primarily applies to the private sector          -3.8486      0.837     -4.599      0.000      -5.489      -2.209\n",
      "BERT_topic4                                      -2.7703      0.866     -3.200      0.001      -4.467      -1.074\n",
      "BERT_topic6                                      -2.6584      0.900     -2.953      0.003      -4.423      -0.894\n",
      "BERT_topic7                                      -2.2990      0.880     -2.613      0.009      -4.023      -0.575\n",
      "gov_category_consolidated_blue_state             -2.3177      0.921     -2.516      0.012      -4.123      -0.512\n",
      "gov_category_consolidated_federal_legislative    -6.2447      0.885     -7.058      0.000      -7.979      -4.511\n",
      "gov_category_consolidated_purple_state           -3.0611      1.008     -3.036      0.002      -5.037      -1.085\n",
      "datedummy                                        -0.0054      0.001     -8.313      0.000      -0.007      -0.004\n",
      "=================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Add interaction terms between 'datedummy' and all gov_category variables\n",
    "import re\n",
    "\n",
    "logit_model = sm.Logit(y, X_no_strat_app_const, missing='drop')\n",
    "logit_results = logit_model.fit()\n",
    "print(\"\\nLogit regression excluding all of the content-related columns, with interaction terms for datedummy on all gov_category vars:\")\n",
    "print(logit_results.summary())\n",
    "\n",
    "# Identify significant variables (p < 0.05) from the previous logit regression\n",
    "signif_vars = logit_results.pvalues[logit_results.pvalues < 0.1].index.tolist()\n",
    "# Remove 'const' if present\n",
    "signif_vars = [var for var in signif_vars if var != 'const']\n",
    "\n",
    "if signif_vars:\n",
    "    X_signif = sm.add_constant(X_no_strat_app[signif_vars])\n",
    "    logit_model_signif = sm.Logit(y, X_signif, missing='drop')\n",
    "    logit_results_signif = logit_model_signif.fit()\n",
    "    print(\"\\nLogit regression using only significant variables (p < 0.05):\")\n",
    "    print(logit_results_signif.summary())\n",
    "else:\n",
    "    print(\"No significant variables (p < 0.05) found in the previous logit regression.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca7a5ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stata_setup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstata_setup\u001b[39;00m\n\u001b[1;32m      3\u001b[0m stata_setup\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Program Files/Stata19\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stata_setup'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
