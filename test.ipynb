{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ec636b-f361-4674-8c3e-c31a237e567e",
   "metadata": {},
   "source": [
    "# Importing AGORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a97ab5-5519-4be4-908d-a91c66e12707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "70a0be43-ede0-4ef6-b47a-a163d6d1ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agora/documents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6b66b570-138e-4533-81a6-aaedd7c04aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This datasets consists of 650 AI-related bills in the USA.\n"
     ]
    }
   ],
   "source": [
    "print(f\"This datasets consists of {len(df)} AI-related bills in the USA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "703183e8-57a0-4eb8-9ae9-00b42feded9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_full_text(agora_id):\n",
    "    text = None\n",
    "    try:\n",
    "        with open(f'agora/fulltext/{agora_id}.txt', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0baf1dd2-fc7d-45f2-8aba-32131bd5d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'agora/fulltext/207.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/494.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/402.txt'\n",
      "[Errno 2] No such file or directory: 'agora/fulltext/29.txt'\n"
     ]
    }
   ],
   "source": [
    "df[\"full_text\"] = df[\"AGORA ID\"].apply(add_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebaf47-c9b2-4646-9dfe-1b5df0029fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is a preview of the first 5 bills in the dataset for reference:\\n\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "46c47ef2-40e3-419c-937c-c0a4dbc07c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('agora_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc66fc-8044-4635-8a88-528b77b9ecf1",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad0cc74-49e9-4f7d-b799-5e7f453d22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agora_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe65441-866c-4ebb-bf1a-331bcf6643e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emilx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2698ad-a6c0-45b0-ab8d-f67ac55383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    # stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return no_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd4e3e9-68ab-492f-92fc-8916d43131cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d89b9fe-67c0-40aa-b4a0-1be9f67b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(df[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498517cb-e485-4621-a4e0-d21a4d7a43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for doc in sample:\n",
    "    try:\n",
    "        sentences += get_sentences(doc)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33231422-e30c-45ef-88dd-82bad9fb6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5f81ea-e033-4191-b815-0368332fd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               vector_size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde24350-ae21-4282-ba21-63b0feaa6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('legal_words.txt') as file:\n",
    "    common_law_terms = file.read()\n",
    "common_law_terms = common_law_terms.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12b5cb0f-638f-49b8-8b85-d42834d75dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_law_similar = list()\n",
    "for term in common_law_terms:\n",
    "    try:\n",
    "        for similar_term, _ in w2v.wv.most_similar(term)[:5]:\n",
    "            common_law_similar.append(similar_term)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a43b779d-2a83-4ca1-9e65-f8904b6d056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_law_terms = common_law_terms + common_law_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618e8db-ca90-4680-8620-9edeaa6151ed",
   "metadata": {},
   "source": [
    "# Pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9edb3d7-52d1-49b2-9616-861a664302f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emilx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ac26413-e772-44e6-b2f7-819406866af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in list(stop_words) + common_law_terms]\n",
    "        bigram = Phrases(tokens, min_count=5, threshold=10)\n",
    "        bigram_mod = Phraser(bigram)\n",
    "        tokens_with_bigrams = bigram_mod[tokens]\n",
    "        return ' '.join(tokens_with_bigrams)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25dba4f0-0b46-41c2-a617-676b2db23d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_text_preprocessed\"] = df[\"full_text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014fb31c-7cdd-475a-956c-848130f850ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"agora_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258f8fb-a218-44a8-826c-220b7a506fdb",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f33b6df-82e0-4bbb-ba08-844656194274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"agora_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c89260-d62e-4d65-81aa-f8257bbae2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "doc_term_matrix = vectorizer.fit_transform(df[\"full_text_preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeb85c92-178d-4fd3-a89b-7b695a68fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'academia', 'academic', 'accelerate', 'access',\n",
       "       'accessible', 'accordance', 'account', 'accountability',\n",
       "       'accuracy', 'achieve', 'acquisition', 'across', 'acting', 'action',\n",
       "       'actions', 'activities', 'activity', 'actors', 'acts', 'actual',\n",
       "       'added', 'adding', 'addition', 'additional', 'address',\n",
       "       'administration', 'administrative', 'administrator', 'adopt',\n",
       "       'adoption', 'ads', 'advance', 'advanced', 'advancing', 'adverse',\n",
       "       'affairs', 'affect', 'affected', 'age', 'agencies', 'agency',\n",
       "       'agencys', 'agreement', 'agreements', 'agriculture', 'ai', 'air',\n",
       "       'aircraft', 'algorithm', 'algorithmic', 'algorithms', 'allow',\n",
       "       'alternative', 'amended', 'america', 'american', 'among', 'amount',\n",
       "       'amounts', 'analysis', 'annex', 'annual', 'another', 'applicable',\n",
       "       'applicant', 'application', 'applications', 'applied', 'apply',\n",
       "       'appointed', 'approach', 'approaches', 'appropriate',\n",
       "       'appropriated', 'appropriations', 'approval', 'approved', 'area',\n",
       "       'areas', 'armed', 'article', 'assembly', 'assess', 'assessment',\n",
       "       'assessments', 'assist', 'assistance', 'assistant', 'associated',\n",
       "       'atmospheric', 'attorney', 'audio', 'authorities', 'authority',\n",
       "       'authorization', 'authorized', 'automated', 'autonomous',\n",
       "       'availability', 'available', 'aviation', 'avoid', 'award',\n",
       "       'awards', 'based', 'basic', 'basis', 'beginning', 'behalf',\n",
       "       'benefit', 'benefits', 'best', 'better', 'bias', 'biological',\n",
       "       'biometric', 'board', 'bodies', 'body', 'border', 'budget',\n",
       "       'build', 'building', 'business', 'businesses', 'campaign',\n",
       "       'candidate', 'capabilities', 'capability', 'capable', 'capacity',\n",
       "       'care', 'carried', 'carry', 'carrying', 'case', 'cases',\n",
       "       'categories', 'cause', 'center', 'centers', 'certain',\n",
       "       'certification', 'chain', 'chains', 'chair', 'challenges',\n",
       "       'change', 'changes', 'chapter', 'characteristics', 'chief',\n",
       "       'child', 'children', 'china', 'circumstances', 'cited', 'civil',\n",
       "       'class', 'classification', 'classified', 'clear', 'clearly',\n",
       "       'cloud', 'code', 'collaboration', 'collected', 'collection',\n",
       "       'colleges', 'commerce', 'commercial', 'commission', 'committees',\n",
       "       'common', 'communication', 'communications', 'communities',\n",
       "       'companies', 'company', 'competent', 'competition', 'competitive',\n",
       "       'complete', 'compliance', 'comply', 'components', 'comprehensive',\n",
       "       'computational', 'computer', 'computing', 'concern', 'concerning',\n",
       "       'concerns', 'conditions', 'conduct', 'conducted', 'conducting',\n",
       "       'conformity', 'congress', 'congressional', 'consent', 'consider',\n",
       "       'consideration', 'considered', 'considers', 'consistent',\n",
       "       'consortium', 'construction', 'construed', 'consult',\n",
       "       'consultation', 'consumer', 'consumers', 'contact', 'containing',\n",
       "       'contains', 'content', 'contents', 'context', 'continue',\n",
       "       'contract', 'contracts', 'control', 'controlled', 'controls',\n",
       "       'cooperation', 'coordinate', 'coordination', 'cost', 'costs',\n",
       "       'could', 'council', 'countries', 'country', 'court', 'covered',\n",
       "       'create', 'created', 'creating', 'creation', 'credit', 'criminal',\n",
       "       'criteria', 'critical', 'current', 'customer', 'customers',\n",
       "       'cyber', 'cybersecurity', 'data', 'database', 'datasets', 'date',\n",
       "       'december', 'deceptive', 'decision', 'decisionmaking', 'decisions',\n",
       "       'deep', 'defense', 'defined', 'definition', 'definitions',\n",
       "       'definitionsin', 'degree', 'delivery', 'demonstrate',\n",
       "       'demonstration', 'department', 'departments', 'depiction',\n",
       "       'deployed', 'deployer', 'deployers', 'deployment', 'described',\n",
       "       'description', 'design', 'designated', 'designed', 'designee',\n",
       "       'detailed', 'detection', 'determination', 'determine',\n",
       "       'determined', 'develop', 'developed', 'developer', 'developers',\n",
       "       'developing', 'development', 'device', 'devices', 'different',\n",
       "       'digital', 'direct', 'directive', 'directly', 'director',\n",
       "       'disclose', 'disclosure', 'discrimination', 'dissemination',\n",
       "       'distributed', 'distribution', 'district', 'diverse', 'diversity',\n",
       "       'document', 'documentation', 'domestic', 'due', 'duties',\n",
       "       'economic', 'education', 'educational', 'effect', 'effective',\n",
       "       'effectively', 'effectiveness', 'effects', 'efficiency', 'efforts',\n",
       "       'eg', 'election', 'electronic', 'eligible', 'emergency',\n",
       "       'emerging', 'employee', 'employees', 'employer', 'employment',\n",
       "       'enable', 'enabling', 'enacted', 'encourage', 'end', 'energy',\n",
       "       'enforcement', 'engage', 'engaged', 'engagement', 'engineering',\n",
       "       'enhance', 'ensure', 'ensuring', 'enter', 'enterprises',\n",
       "       'entities', 'entity', 'environment', 'environmental',\n",
       "       'environments', 'equipment', 'equity', 'establish', 'established',\n",
       "       'establishing', 'establishment', 'et', 'etc', 'ethical', 'ethics',\n",
       "       'eu', 'european', 'evaluate', 'evaluation', 'evaluations', 'event',\n",
       "       'every', 'evidence', 'example', 'except', 'executive', 'existing',\n",
       "       'expand', 'expected', 'experience', 'expertise', 'experts',\n",
       "       'export', 'extent', 'external', 'facial', 'facilitate',\n",
       "       'facilities', 'facility', 'factors', 'federal', 'feedback',\n",
       "       'field', 'fields', 'final', 'financial', 'findings', 'fire',\n",
       "       'first', 'fiscal', 'five', 'focus', 'following', 'follows', 'food',\n",
       "       'force', 'foreign', 'form', 'forth', 'foundation', 'framework',\n",
       "       'frameworks', 'frontier', 'full', 'function', 'functions', 'fund',\n",
       "       'fundamental', 'funding', 'funds', 'future', 'gender', 'general',\n",
       "       'generalnot', 'generalpurpose', 'generalthe', 'generate',\n",
       "       'generated', 'generation', 'generative', 'given', 'global',\n",
       "       'goals', 'good', 'goods', 'governance', 'government',\n",
       "       'governments', 'grant', 'grants', 'greater', 'group', 'groups',\n",
       "       'guidance', 'guide', 'guidelines', 'hardware', 'harm', 'harmful',\n",
       "       'harms', 'head', 'heads', 'health', 'help', 'high', 'higher',\n",
       "       'highrisk', 'hiring', 'homeland', 'human', 'identifiable',\n",
       "       'identification', 'identified', 'identify', 'identifying',\n",
       "       'identity', 'image', 'images', 'impact', 'impacts', 'implement',\n",
       "       'implementation', 'implemented', 'implementing', 'important',\n",
       "       'improve', 'improving', 'include', 'included', 'includes',\n",
       "       'including', 'increase', 'independent', 'individual',\n",
       "       'individuals', 'industrial', 'industries', 'industry', 'influence',\n",
       "       'inform', 'information', 'infrastructure', 'initial', 'initiative',\n",
       "       'innovation', 'innovative', 'input', 'inserting', 'institute',\n",
       "       'institution', 'institutions', 'insurance', 'integrated',\n",
       "       'integration', 'integrity', 'intellectual', 'intelligent',\n",
       "       'intended', 'interagency', 'interest', 'interests', 'internal',\n",
       "       'international', 'internet', 'intimate', 'investigation',\n",
       "       'investment', 'involved', 'involving', 'issue', 'issued', 'issues',\n",
       "       'january', 'jurisdiction', 'key', 'knowledge', 'known', 'labor',\n",
       "       'laboratories', 'language', 'large', 'law', 'laws', 'lead',\n",
       "       'leadership', 'learning', 'least', 'legal', 'legislative', 'less',\n",
       "       'level', 'levels', 'leverage', 'liberties', 'life', 'lifecycle',\n",
       "       'like', 'likely', 'limitations', 'limited', 'list', 'listed',\n",
       "       'literacy', 'loan', 'local', 'located', 'machine', 'made',\n",
       "       'maintain', 'maintenance', 'major', 'make', 'makes', 'making',\n",
       "       'manage', 'management', 'managing', 'manner', 'manufacturing',\n",
       "       'many', 'market', 'material', 'materially', 'materials', 'matter',\n",
       "       'matters', 'meaning', 'means', 'measure', 'measures', 'mechanism',\n",
       "       'mechanisms', 'media', 'medical', 'meet', 'meeting', 'members',\n",
       "       'memorandum', 'mental', 'methods', 'metrics', 'military',\n",
       "       'minimum', 'mission', 'misuse', 'mitigate', 'mitigation', 'model',\n",
       "       'modeling', 'models', 'monitor', 'monitoring', 'months',\n",
       "       'multiple', 'nairr', 'name', 'national', 'natural', 'nature',\n",
       "       'necessary', 'needed', 'needs', 'network', 'networks', 'new',\n",
       "       'nonprofit', 'note', 'notice', 'notification', 'notified',\n",
       "       'nuclear', 'number', 'objectives', 'obligations', 'obtain',\n",
       "       'obtained', 'office', 'officer', 'officers', 'offices', 'official',\n",
       "       'officials', 'omb', 'one', 'ongoing', 'online', 'open',\n",
       "       'operating', 'operation', 'operational', 'operations', 'operator',\n",
       "       'operators', 'opportunities', 'opportunity', 'order',\n",
       "       'organization', 'organizations', 'others', 'otherwise', 'outcomes',\n",
       "       'output', 'outputs', 'overall', 'oversight', 'paragraph',\n",
       "       'paragraphs', 'part', 'participate', 'participation', 'particular',\n",
       "       'parties', 'partners', 'partnership', 'partnerships', 'party',\n",
       "       'patient', 'pay', 'payment', 'penalties', 'penalty', 'people',\n",
       "       'peoples', 'percent', 'perform', 'performance', 'period', 'person',\n",
       "       'personal', 'personnel', 'persons', 'physical', 'pilot', 'place',\n",
       "       'plan', 'planning', 'plans', 'platform', 'platforms', 'point',\n",
       "       'policies', 'policy', 'political', 'position', 'positions',\n",
       "       'possible', 'potential', 'power', 'powers', 'practicable',\n",
       "       'practice', 'practices', 'prescribed', 'president', 'prevent',\n",
       "       'primary', 'principles', 'prior', 'privacy', 'private',\n",
       "       'procedure', 'procedures', 'process', 'processes', 'processing',\n",
       "       'procurement', 'produced', 'product', 'production', 'products',\n",
       "       'professional', 'program', 'programs', 'progress', 'prohibited',\n",
       "       'prohibition', 'project', 'projects', 'promote', 'promoting',\n",
       "       'property', 'proposed', 'protect', 'protected', 'protection',\n",
       "       'protections', 'provenance', 'provide', 'provided', 'provider',\n",
       "       'providers', 'provides', 'providing', 'provision', 'provisions',\n",
       "       'public', 'publication', 'publicly', 'published', 'purpose',\n",
       "       'purposes', 'pursuant', 'qualified', 'quality', 'quantum',\n",
       "       'questions', 'range', 'rd', 'read', 'real', 'reasonable',\n",
       "       'reasonably', 'receive', 'received', 'recognition',\n",
       "       'recommendation', 'recommendations', 'record', 'records', 'reduce',\n",
       "       'reference', 'referred', 'regard', 'regarding', 'regulation',\n",
       "       'regulations', 'regulatory', 'related', 'relating', 'release',\n",
       "       'relevant', 'reliability', 'relief', 'report', 'reporting',\n",
       "       'reportnot', 'reports', 'representative', 'representatives',\n",
       "       'republic', 'request', 'requests', 'require', 'required',\n",
       "       'requirement', 'requirements', 'requires', 'research',\n",
       "       'researchers', 'resilience', 'resource', 'resources', 'respect',\n",
       "       'response', 'responsibilities', 'responsibility', 'responsible',\n",
       "       'result', 'results', 'review', 'right', 'rights', 'risk', 'risks',\n",
       "       'role', 'roles', 'rule', 'rules', 'safe', 'safeguards', 'safety',\n",
       "       'scale', 'school', 'science', 'sciences', 'scientific', 'scope',\n",
       "       'score', 'screening', 'sec', 'secretary', 'sections', 'sector',\n",
       "       'sectors', 'secure', 'security', 'seek', 'select', 'senior',\n",
       "       'sensitive', 'seq', 'serve', 'service', 'services', 'set', 'sets',\n",
       "       'sexual', 'share', 'shared', 'sharing', 'short', 'significant',\n",
       "       'similar', 'skills', 'small', 'smart', 'social', 'societal',\n",
       "       'society', 'software', 'solutions', 'source', 'sources', 'space',\n",
       "       'special', 'specific', 'specified', 'spectrum', 'speech', 'sports',\n",
       "       'staff', 'stakeholders', 'standard', 'standards', 'state',\n",
       "       'statement', 'status', 'stem', 'storage', 'strategic',\n",
       "       'strategies', 'strategy', 'strengthen', 'striking', 'structure',\n",
       "       'students', 'study', 'subdivision', 'subject', 'submission',\n",
       "       'submit', 'submitted', 'subsection', 'substantially', 'sufficient',\n",
       "       'summary', 'supply', 'support', 'supporting', 'surveillance',\n",
       "       'synthetic', 'system', 'systems', 'table', 'take', 'taken',\n",
       "       'taking', 'talent', 'task', 'tasks', 'team', 'teams', 'technical',\n",
       "       'techniques', 'technological', 'technologies', 'technology',\n",
       "       'term', 'terms', 'test', 'testing', 'text', 'thereof', 'third',\n",
       "       'threat', 'threats', 'throughout', 'time', 'timely', 'title',\n",
       "       'tool', 'tools', 'trade', 'train', 'training', 'transaction',\n",
       "       'transfer', 'transparency', 'transportation', 'treatment',\n",
       "       'tribal', 'trust', 'trustworthy', 'two', 'type', 'types',\n",
       "       'understand', 'understanding', 'union', 'unlawful', 'unless',\n",
       "       'unmanned', 'update', 'updated', 'upon', 'us', 'usc', 'use',\n",
       "       'used', 'user', 'users', 'uses', 'using', 'utilization',\n",
       "       'validation', 'value', 'vehicle', 'vehicles', 'vendor',\n",
       "       'verification', 'video', 'vii', 'violation', 'violations',\n",
       "       'visual', 'voice', 'voluntary', 'vulnerabilities', 'wagering',\n",
       "       'water', 'way', 'ways', 'weapon', 'weather', 'website', 'well',\n",
       "       'whole', 'whose', 'within', 'without', 'work', 'worker', 'workers',\n",
       "       'workforce', 'working', 'world', 'written', 'year', 'years'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d5eeb9c-ca2b-4fa2-8176-a19b68246889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 5, 0],\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 6, 1, 1],\n",
       "       ...,\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4a41d75-2490-40da-87a8-2f9722c9b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=200,\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    batch_size=128,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c328e794-e34b-496d-9660-90a11054b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = lda.fit_transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b81865c-1dc5-4eee-9e16-8d471b56659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "276a323c-6cc8-4e6d-9a8e-9e3d79e910d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words in each topic:\n",
      "\n",
      "Topic 1:\n",
      "information, covered, state, person, commission, means, including, individual, service, data\n",
      "\n",
      "Topic 2:\n",
      "ai, systems, data, use, system, risks, security, development, including, safety\n",
      "\n",
      "Topic 3:\n",
      "secretary, national, including, subsection, director, research, technology, program, defense, paragraph\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop words in each topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-10-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    print(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93ba190-0a50-4a98-9e21-b6c8ad761144",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = [f'Topic_{i+1}' for i in range(n_topics)]\n",
    "df_topics = pd.DataFrame(lda_output, columns=topic_columns)\n",
    "df = pd.concat([df, df_topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ae27df1-faec-484a-a9bb-0fc5b22fd151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example documents for each topic:\n",
      "\n",
      "Top documents for Topic 1:\n",
      "\\Text: enacted state utah b amended read b definitions used chapter child sexual abuse material means visual depiction including live performance photograph film video picture computer computergenerated imag...\n",
      "Topic 1 probability: 0.999\n",
      "\\Text: relating elections amending enacting sections campaign reporting adding disclaimer requirements advertisements containing materially deceptive media creating crime distributing entering agreement anot...\n",
      "Topic 1 probability: 0.999\n",
      "\\Text: short title cited tools address known exploitation immobilizing technological deepfakes websites networks take sec criminal prohibition intentional disclosure nonconsensual intimate visual depictions ...\n",
      "Topic 1 probability: 0.999\n",
      "\n",
      "Top documents for Topic 2:\n",
      "\\Text: hiroshima process international code conduct organizations developing advanced ai systems basis international guiding principles organizations developing advanced ai systems international code conduct...\n",
      "Topic 2 probability: 0.999\n",
      "\\Text: background tangible global leadership european union provide scalable sciencebased methods advance trustworthy approaches ai serve people responsible equitable beneficial ways effective risk managemen...\n",
      "Topic 2 probability: 0.999\n",
      "\\Text: general assembly opening declarations omitted resolves bridge digital divides within countries resolves promote safe secure trustworthy systems accelerate progress towards full realization agenda sust...\n",
      "Topic 2 probability: 0.999\n",
      "\n",
      "Top documents for Topic 3:\n",
      "\\Text: sec strengthening mobility revolutionizing transportation grant program definitionsin eligible entitythe term eligible entity means state b political subdivision state c tribal government public trans...\n",
      "Topic 3 probability: 0.999\n",
      "\\Text: promote leadership technical standards directing national institute standards technology department state take certain actions encourage enable participation developing standards specifications critic...\n",
      "Topic 3 probability: 0.999\n",
      "\\Text: sec centers excellence food agriculture conservation trade usc amended striking subsections b c inserting following centers excellence generalthe secretary agriculture establish least one center excel...\n",
      "Topic 3 probability: 0.998\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample documents for each topic:\")\n",
    "for topic_idx in range(n_topics):\n",
    "    print(f\"\\nTop documents for Topic {topic_idx + 1}:\")\n",
    "    top_docs = df.nlargest(3, f'Topic_{topic_idx+1}')\n",
    "    for idx, row in top_docs.iterrows():\n",
    "        print(f\"\\Text: {row['full_text_preprocessed'][:200]}...\")\n",
    "        print(f\"Topic {topic_idx + 1} probability: {row[f'Topic_{topic_idx+1}']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef2ab5-44dd-435c-8e51-29d7c8fc4c4b",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22cd14ea-e9c1-43ea-9af1-909c2a8ff785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilx\\anaconda3\\envs\\nlp_lss\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "485eee2a-6b9a-4543-8425-0ef17b89ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean', prediction_data=True)\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    min_topic_size=10,     # Merges tiny topics into larger ones, when set to 20 only 2 topics\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abdbdad7-5e8d-4638-b6fc-fc4d12a83541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 12:20:26,124 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 21/21 [01:27<00:00,  4.14s/it]\n",
      "2025-05-28 12:21:54,834 - BERTopic - Embedding - Completed ✓\n",
      "2025-05-28 12:21:54,837 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-28 12:21:58,363 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-28 12:21:58,366 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-05-28 12:21:58,557 - BERTopic - Cluster - Completed ✓\n",
      "2025-05-28 12:21:58,572 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-05-28 12:22:00,691 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(df['full_text_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8984981-4e57-4e50-aab2-5af9bec8c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words for each topic:\n",
      "\n",
      "Topic 0:\n",
      "defense, education, director, program, research, secretary, subsection, national, including, paragraph\n",
      "\n",
      "Topic 1:\n",
      "ai, systems, system, data, use, article, security, risks, model, development\n",
      "\n",
      "Topic 2:\n",
      "energy, research, secretary, national, program, including, weather, development, technologies, data\n",
      "\n",
      "Topic 3:\n",
      "person, individual, election, media, image, audio, means, visual, sexual, video\n",
      "\n",
      "Topic 4:\n",
      "automated, system, data, decision, information, use, state, employer, used, systems\n",
      "\n",
      "Topic 5:\n",
      "covered, foreign, entity, president, secretary, security, regulations, term, national, activity\n",
      "\n",
      "Topic 6:\n",
      "health, care, plan, services, medical, patient, ai, use, program, benefits\n",
      "\n",
      "Topic 7:\n",
      "commission, agency, council, data, digital, criticalimpact, chief, government, state, including\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop words for each topic:\")\n",
    "for topic_id in topic_model.get_topics():\n",
    "    if topic_id != -1:  # Skip the outlier topic (-1)\n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        print(f\"\\nTopic {topic_id}:\")\n",
    "        print(\", \".join([word for word, _ in words[:10]]))  # Show top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9cef1eca-5d54-414b-9041-382fb256af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_probs = pd.DataFrame(probs, columns=['BERT_topic0', 'BERT_topic1', 'BERT_topic2', 'BERT_topic3', 'BERT_topic4', 'BERT_topic5', 'BERT_topic6', 'BERT_topic7'])\n",
    "df = pd.concat([df, BERT_probs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b85e18c-448e-463e-90a9-41c7499eb07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   9,  20,  22,  25,  26,  34,  35,  36,  37,  38,  39,  40,\n",
       "        41,  42,  43,  44,  45,  53,  54,  56,  82,  83,  91,  92,  93,\n",
       "       101, 105, 107, 116, 117, 123, 132, 137, 143, 149, 167, 174, 175,\n",
       "       176, 178, 180, 181, 182, 183, 184, 185, 187, 188, 189, 195, 198,\n",
       "       210, 218, 220, 222, 225, 229, 232, 234, 238, 239, 243, 246, 248,\n",
       "       251, 252, 253, 261, 269, 271, 272, 283, 290, 291, 293, 296, 297,\n",
       "       298, 301, 302, 306, 307, 310, 315, 316, 317, 318, 321, 324, 325,\n",
       "       337, 340, 341, 343, 344, 353, 357, 358, 361, 373, 374, 381, 386,\n",
       "       390, 399, 400, 402, 411, 414, 416, 417, 423, 425, 427, 430, 432,\n",
       "       437, 442, 443, 448, 450, 452, 454, 456, 460, 462, 464, 467, 470,\n",
       "       472, 474, 476, 478, 479, 486, 488, 490, 495, 498, 499, 502, 503,\n",
       "       504, 505, 507, 508, 509, 514, 515, 516, 517, 518, 519, 527, 532,\n",
       "       533, 534, 535, 537, 547, 548, 552, 554, 555, 558, 559, 566, 568,\n",
       "       569, 570, 582, 583, 584, 585, 586, 588, 589, 600, 602, 605, 606,\n",
       "       611, 613, 614, 615, 616, 617, 618, 621, 623, 641, 645, 646, 647],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_topic_indices = np.where(np.array(topics) == -1)[0]\n",
    "dummy_topic_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d150819-3491-4925-892d-d33c7cfb2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(dummy_topic_indices)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "20fa6756-3381-4c02-914c-b2128df1124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('agora_topic_probabilities.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_lss]",
   "language": "python",
   "name": "conda-env-nlp_lss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
